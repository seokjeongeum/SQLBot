{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48012ee4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ANALYLZE\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import _jsonnet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools \n",
    "import transformers\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.utils.analysis import cal_attention_flow\n",
    "from search import read_data, match, show_results\n",
    "from run_all import test_example, load_model, TestInfo\n",
    "from ratsql.utils.relation_names import RELATION_NAMES\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859f646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for attention flow\n",
    "def draw_attention_graph(adjmat, labels_to_index, n_layers, length):\n",
    "    A = adjmat\n",
    "    G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "    pos = {}\n",
    "    label_pos = {}\n",
    "    for i in np.arange(n_layers+1):\n",
    "        for k_f in np.arange(length):\n",
    "            pos[i*length+k_f] = ((i+0.5)*2, length - k_f)\n",
    "            label_pos[i*length+k_f] = (i*2, length - k_f)\n",
    "\n",
    "    index_to_labels = {}\n",
    "    for key in labels_to_index:\n",
    "        index_to_labels[labels_to_index[key]] = key.split(\"_\")[-1]\n",
    "        if labels_to_index[key] >= length:\n",
    "            index_to_labels[labels_to_index[key]] = ''\n",
    "\n",
    "    #plt.figure(1,figsize=(20,12))\n",
    "\n",
    "    nx.draw_networkx_nodes(G,pos,node_color='green', node_size=50)\n",
    "    nx.draw_networkx_labels(G,pos=label_pos, labels=index_to_labels, font_size=10)\n",
    "\n",
    "    all_weights = []\n",
    "    #4 a. Iterate through the graph nodes to gather all the weights\n",
    "    for (node1,node2,data) in G.edges(data=True):\n",
    "        all_weights.append(data['weight']) #we'll use this when determining edge thickness\n",
    "\n",
    "    #4 b. Get unique weights\n",
    "    unique_weights = list(set(all_weights))\n",
    "\n",
    "    #4 c. Plot the edges - one by one!\n",
    "    for weight in unique_weights:\n",
    "        #4 d. Form a filtered list with just the weight you want to draw\n",
    "        weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
    "        #4 e. I think multiplying by [num_nodes/sum(all_weights)] makes the graphs edges look cleaner\n",
    "        \n",
    "        w = weight #(weight - min(all_weights))/(max(all_weights) - min(all_weights))\n",
    "        width = w\n",
    "        nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,width=width, edge_color='darkblue')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def get_adjmat(mat, input_tokens):\n",
    "    n_layers, length, _ = mat.shape\n",
    "    adj_mat = np.zeros(((n_layers+1)*length, (n_layers+1)*length))\n",
    "    labels_to_index = {}\n",
    "    for k in np.arange(length):\n",
    "        labels_to_index[str(k)+\"_\"+input_tokens[k]] = k\n",
    "\n",
    "    for i in np.arange(1,n_layers+1):\n",
    "        for k_f in np.arange(length):\n",
    "            index_from = (i)*length+k_f\n",
    "            label = \"L\"+str(i)+\"_\"+str(k_f)\n",
    "            labels_to_index[label] = index_from\n",
    "            for k_t in np.arange(length):\n",
    "                index_to = (i-1)*length+k_t\n",
    "                adj_mat[index_from][index_to] = mat[i-1][k_f][k_t]\n",
    "                \n",
    "    return adj_mat, labels_to_index \n",
    "\n",
    "def add_residual_connection(attentions_mat):\n",
    "    \"\"\"\n",
    "    attentions_mat: (layer_num, head_num, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    res_att_mat = attentions_mat.sum(axis=1)/attentions_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "    return res_att_mat\n",
    "\n",
    "def compute_flows(G, labels_to_index, input_nodes, length):\n",
    "    print(type(G))\n",
    "    print(type(labels_to_index))\n",
    "    print(type(input_nodes))\n",
    "    number_of_nodes = len(labels_to_index)\n",
    "    flow_values=np.zeros((number_of_nodes,number_of_nodes))\n",
    "    for key in labels_to_index:\n",
    "        if key not in input_nodes:\n",
    "            current_layer = int(labels_to_index[key] / length)\n",
    "            pre_layer = current_layer - 1\n",
    "            u = labels_to_index[key]\n",
    "            for inp_node_key in input_nodes:\n",
    "                v = labels_to_index[inp_node_key]\n",
    "                flow_value = nx.maximum_flow_value(G,u,v, flow_func=nx.algorithms.flow.edmonds_karp)\n",
    "                flow_values[u][pre_layer*length+v ] = flow_value\n",
    "            flow_values[u] /= flow_values[u].sum()\n",
    "            \n",
    "    return flow_values\n",
    "\n",
    "def convert_adjmat_tomats(adjmat, n_layers, l):\n",
    "    mats = np.zeros((n_layers,l,l))\n",
    "    for i in np.arange(n_layers):\n",
    "        mats[i] = adjmat[(i+1)*l:(i+2)*l,i*l:(i+1)*l]\n",
    "    return mats\n",
    "\n",
    "def plot_attention_heatmap(att, s_position, t_positions, sentence):\n",
    "      print(att[:,s_position, t_positions].shape)\n",
    "      for idx, values in enumerate(att[:,s_position, t_positions]):\n",
    "          print(f\"idx:{idx} values:{values}\")\n",
    "      cls_att = np.flip(att[:,s_position, t_positions], axis=0)\n",
    "      xticklb = input_tokens= list(itertools.compress(['<cls>']+sentence.split(), [i in t_positions for i in np.arange(len(sentence)+1)]))\n",
    "      yticklb = [str(i) if i%2 ==0 else '' for i in np.arange(att.shape[0],0, -1)]\n",
    "      ax = sns.heatmap(cls_att, xticklabels=xticklb, yticklabels=yticklb, cmap=\"YlOrRd\")\n",
    "      return ax\n",
    "\n",
    "def get_attention_flow(tokens, attentions_mat):\n",
    "    \"\"\"\n",
    "    attentions_mat: (layer_num, head_num, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    layer_num, head_num, seq_len, seq_len = attentions_mat.shape\n",
    "    res_att_mat = add_residual_connection(attentions_mat)\n",
    "    \n",
    "    # Convert matrices to adjacency matrices\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=tokens)\n",
    "\n",
    "    #@title plot the attention graph\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    # res_adj_mat and res_labels_to_index are the results of get_adjmat()\n",
    "    res_G = draw_attention_graph(res_adj_mat,res_labels_to_index, n_layers=layer_num, length=seq_len)\n",
    "\n",
    "    # Calculate the flow values\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < seq_len:\n",
    "            input_nodes.append(key)\n",
    "    flow_values = compute_flows(res_G, res_labels_to_index, input_nodes, length=seq_len)\n",
    "    \n",
    "    # Convert adjacency matrix to matrices\n",
    "    flow_att_mat = convert_adjmat_tomats(flow_values, n_layers=layer_num, l=seq_len)\n",
    "\n",
    "    return flow_att_mat\n",
    "\n",
    "def draw_attention_flow(sentence, target_idx, flow_att_mat, indices_to_inspect=None):\n",
    "    # Select source indices to inspect\n",
    "    if indices_to_inspect is None:\n",
    "        indices_to_inspect = list(range(sentence))\n",
    "    \n",
    "    # Plot the attention flow\n",
    "    plt.figure(1,figsize=(3,6))\n",
    "    plot_attention_heatmap(flow_att_mat, target_idx, t_positions=indices_to_inspect, sentence=sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6348e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "project_dir = \"/home/hkkang/NL2QGM\"\n",
    "\n",
    "# Trained model seeds\n",
    "model_seeds = [0, 2, 3]\n",
    "\n",
    "def get_info_paths(model_type, target_model_seed):\n",
    "    bert_template = \"logdir/spider_bert_run_no_join_cond_seed_{}/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed={},join_cond=false/ie_dirs/bert_run_true_1-step_41600-eval.json\"\n",
    "    electra_template = \"logdir/spider_electra_run_no_join_cond_seed_{}/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed={},join_cond=false/ie_dirs/electra_run_true_1-step_41600-eval.json\"\n",
    "    glove_template = \"logdir/spider_glove_run_no_join_cond_seed_{}/bs=24,lr=7.4e-04,end_lr=0e0,seed={},join_cond=false/ie_dirs/glove_run_true_1-step_41600-eval.json\"\n",
    "    if model_type == 'bert':\n",
    "        template = bert_template\n",
    "    elif model_type == 'electra':\n",
    "        template = electra_template\n",
    "    elif model_type == 'glove':\n",
    "        template = glove_template\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    template = os.path.join(project_dir, template)\n",
    "\n",
    "    if model_type == 'electra':\n",
    "        if target_model_seed == 0:\n",
    "            template = template.replace('41600', '18000')\n",
    "        elif target_model_seed == 2:\n",
    "            template = template.replace('41600', '25000')\n",
    "        elif target_model_seed == 3:\n",
    "            template = template.replace('41600', '26000')\n",
    "        model_seeds = [0,2,3]\n",
    "    elif model_type == 'bert':\n",
    "        if target_model_seed == 0:\n",
    "            template = template.replace('41600', '78000')\n",
    "        model_seeds = [0,2,3]\n",
    "    elif model_type == 'glove':\n",
    "        if target_model_seed == 0:\n",
    "            template = template.replace('41600', '20000')\n",
    "        elif target_model_seed == 1:\n",
    "            template = template.replace('41600', '18000')\n",
    "        elif target_model_seed == 2:\n",
    "            template = template.replace('41600', '18000')\n",
    "        model_seeds = [0,1,2]\n",
    "\n",
    "    eval_paths = []\n",
    "    for seed in model_seeds:\n",
    "        eval_paths += [template.format(seed, seed)]\n",
    "\n",
    "    infer_paths = [path.replace(\"-eval.json\", \"-infer.jsonl\") for path in eval_paths]\n",
    "    debug_paths = [path.replace(\"-eval.json\", \"-debug.jsonl\") for path in eval_paths]\n",
    "    return infer_paths, debug_paths, eval_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211f9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables.json\n",
    "tables_path = os.path.join(project_dir, \"data/spider/tables.json\")\n",
    "\n",
    "with open(tables_path) as f:\n",
    "    dbs = {item['db_id']: item for item in json.load(f)}\n",
    "\n",
    "def get_column_names(db_id):\n",
    "    db = dbs[db_id]\n",
    "    tables = db['table_names']\n",
    "    columns = []\n",
    "    for table_id, column_name in db['column_names']:\n",
    "        column_name = column_name.replace(' ', '_')\n",
    "        if table_id == -1:\n",
    "            columns.append(column_name)\n",
    "        else:\n",
    "            columns.append(f\"{tables[table_id].replace(' ', '_')}.{column_name}\")\n",
    "    return columns\n",
    "\n",
    "def get_table_names(db_id):\n",
    "    db = dbs[db_id]\n",
    "    tables = db['table_names']\n",
    "    return [table.replace(' ', '_') for table in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f68e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_json_custom(path):\n",
    "    result = json.load(open(path))['per_item']\n",
    "    return result\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        results = [json.loads(line) for line in f.readlines()]\n",
    "    return results\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_info(eval_path, debug_path, infer_path):\n",
    "    eval_result = load_json_custom(eval_path.replace('.json', '-testing.json'))[0]\n",
    "    debug_cache = load_pickle(debug_path.replace('.jsonl', '-testing.pkl'))[0]\n",
    "    debug_result = load_jsonl(debug_path.replace('.jsonl', '-testing.jsonl'))[0]\n",
    "    infer_result = load_jsonl(infer_path.replace('.jsonl', '-testing.jsonl'))[0]\n",
    "\n",
    "    return eval_result, debug_result, debug_cache, infer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4af37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(mma, target_labels, source_labels, title=None, decimal=4, save_as_file=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mma: nxn weight matrix\n",
    "        source_labels: List of column labels\n",
    "        target_labels: List of row labels\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20,20), dpi=100)\n",
    "    im = ax.imshow(mma)\n",
    "    \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(mma.shape[1]), minor=False) # mma.shape[1] = target seq 길이\n",
    "    ax.set_yticks(np.arange(mma.shape[0]), minor=False) # mma.shape[0] = input seq 길이\n",
    "   \n",
    "    # source words -> column labels\n",
    "    ax.set_xticklabels(source_labels, minor=False)\n",
    "    # target words -> row labels\n",
    "    ax.set_yticklabels(target_labels, minor=False)\n",
    "  \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    " \n",
    "    plt.xticks(rotation=65)\n",
    " \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(mma)):\n",
    "        for j in range(len(mma[0])):\n",
    "            if decimal == 4:\n",
    "                text = ax.text(j, i, \"{:.4f}\".format(mma[i, j].item()), ha=\"center\", va=\"center\", color=\"k\")\n",
    "            else:\n",
    "                text = ax.text(j, i, \"{:.3f}\".format(mma[i, j].item()), ha=\"center\", va=\"center\", color=\"k\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    if save_as_file:\n",
    "        plt.savefig(f'{title}.png', dpi=100)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46369ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_schema_entity_indices(eval_result, debug_result, debug_cache, infer_result):\n",
    "    db_id = eval_result['db_id']\n",
    "    questions = debug_cache['question']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    tokens = questions + columns + tables\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx < len(questions):\n",
    "            continue\n",
    "        if idx < (len(questions) + len(columns)):\n",
    "            entity_type = 'column'\n",
    "        else:\n",
    "            entity_type = 'table'\n",
    "        print(f\"idx:{idx}\\t{entity_type}:\\t{token}\")\n",
    "        \n",
    "def get_vector_norm(vector):\n",
    "    try:\n",
    "        return np.linalg.norm(vector)\n",
    "    except:\n",
    "        return torch.linalg.norm(vector)\n",
    "    \n",
    "def show_basic_details(eval_result, debug_result, debug_cache, infer_result):\n",
    "    db_id = eval_result['db_id']\n",
    "    nl = infer_result['question_toks']\n",
    "    pred = eval_result['predicted']\n",
    "    gold = eval_result['gold']\n",
    "    decode_history = debug_result['history']\n",
    "    correct = eval_result['exact']\n",
    "    print(f\"Is_correct:{bool(correct)}\")\n",
    "    print(f\"DB_id:{db_id}\")\n",
    "    print(f\"NL:{nl}\")\n",
    "    print(f\"Pred:{pred}\")\n",
    "    print(f\"Gold:{gold}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ae8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_encoder_details(eval_result, debug_result, debug_cache, infer_result, src_idx, des_idx, inspect_layer_num, target_key, top_k=10, prefix=None):\n",
    "    def get_layer_idx(key):\n",
    "        if 'layer' in key:\n",
    "            return int(key.split('_')[1])\n",
    "        return None\n",
    "        \n",
    "    def to_str(value):\n",
    "        if type(value) == str:\n",
    "            return value\n",
    "        elif type(value) == list:\n",
    "            return ' '.join(value)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Bad type:{value}\")\n",
    "\n",
    "    def print_top_7(joint_weight_matrix, input_tokens):\n",
    "        \"\"\"\n",
    "        joint_weight_matrix: [input_num, output_num]\n",
    "        \"\"\"\n",
    "        top_7_values, top_7_indices = torch.topk(joint_weight_matrix, k=7)\n",
    "        for cur_idx, (top_7_index, top_7_value) in enumerate(zip(top_7_indices, top_7_values)):\n",
    "            print(f\"idx:{cur_idx} word:{input_tokens[cur_idx]}\")\n",
    "            for top_idx, (item_idx, weight_value) in enumerate(zip(top_7_index, top_7_value)):\n",
    "                print(f\"\\ttop{top_idx+1}_idx:{item_idx} (word:{input_tokens[item_idx]}) {weight_value}\")\n",
    "\n",
    "    def print_top_7_for_all_layer_for_target_word(joint_weight_matrix, input_tokens, target_word_idx):\n",
    "        \"\"\"\n",
    "        joint_weight_matrix: [layer_num, input_num, output_num]\n",
    "        \"\"\"\n",
    "        joint_weight_matrix = torch.tensor(joint_weight_matrix)[:,target_word_idx,:]\n",
    "        print(joint_weight_matrix.shape)\n",
    "        top_7_values, top_7_indices = torch.topk(joint_weight_matrix, k=7)\n",
    "        for cur_idx, (top_7_index, top_7_value) in enumerate(zip(top_7_indices, top_7_values)):\n",
    "            print(f\"idx:{cur_idx} word:{input_tokens[target_word_idx]}\")\n",
    "            for top_idx, (item_idx, weight_value) in enumerate(zip(top_7_index, top_7_value)):\n",
    "                print(item_idx)\n",
    "                print(f\"\\ttop{top_idx+1}_idx:{item_idx} (word:{input_tokens[item_idx]}) {weight_value}\")\n",
    "\n",
    "                \n",
    "    def print_float(prefix, tensor):\n",
    "        assert len(tensor.shape) == 1, f\"bad: {tensor.shape}\"\n",
    "        print(prefix)\n",
    "        print(f'\\t', end='')\n",
    "        avg = sum(tensor) #/ len(tensor)\n",
    "        print(\"{:.3f}  |\\t\".format(avg), end='')\n",
    "        for value in tensor:\n",
    "            print(\"{:.3f}\\t\".format(value), end='')\n",
    "        print(\"\")\n",
    "\n",
    "    def show_attention_infos(debug_cache, tokens, inspect_layer_num, src_idx, des_idx):\n",
    "        # Show \\alpha_{ij}, e_{ij}, embedding_sim, relation_bias_sim\n",
    "        attn_probs =  debug_cache[f'layer_{inspect_layer_num}_attn_probs']\n",
    "        sim_logits =  debug_cache[f'layer_{inspect_layer_num}_sim_logits']\n",
    "        word_sim = debug_cache[f'layer_{inspect_layer_num}_emb_sim_logits']\n",
    "        bias_sim = debug_cache[f'layer_{inspect_layer_num}_bias_sim_logits']\n",
    "        relation_id = debug_cache['relation'][src_idx][des_idx]\n",
    "        relation_name = RELATION_NAMES[relation_id]\n",
    "        print(f\"layer_num:{inspect_layer_num}\")\n",
    "        print(f\"i:{src_idx} ({tokens[src_idx]}) j:{des_idx} ({tokens[des_idx]}) r:{relation_id} ({relation_name})\")\n",
    "        print_float(\"softmax_prob:\", attn_probs[:, src_idx, des_idx])\n",
    "        print_float(\"sim_score (word+bias):\", sim_logits[:, src_idx, des_idx])\n",
    "        print_float(\"word_sim:\", word_sim[:, src_idx, des_idx])\n",
    "        print_float(\"bias_sim:\", bias_sim[:, src_idx, des_idx])\n",
    "        print_float(\"word_sim_prob:\", torch.softmax(torch.tensor(word_sim), dim=-1)[:, src_idx, des_idx])\n",
    "        print_float(\"bias_sim_prob:\", torch.softmax(torch.tensor(bias_sim), dim=-1)[:, src_idx, des_idx])\n",
    "        print(\"\")\n",
    "\n",
    "    def show_top_attention_infos_for_layer(target_inspect_layer_num, debug_cache, tokens, inspect_layer_num, src_idx, des_idx):\n",
    "        attn_probs = torch.tensor(debug_cache[f'layer_{target_inspect_layer_num}_attn_probs'])\n",
    "        avg_attn_probs = attn_probs.mean(dim=0)\n",
    "        if des_idx is None:\n",
    "            if top_k:\n",
    "                # Find top-10 most similary des_indices\n",
    "                top_values, top_indices = torch.topk(avg_attn_probs[src_idx], k=min(top_k, len(avg_attn_probs)))\n",
    "                des_indices = top_indices\n",
    "                # Show attention info for all target destination indices\n",
    "                for top_idx, target_des_idx in enumerate(des_indices):\n",
    "                    print(f\"Top {top_idx+1}\")\n",
    "                    show_attention_infos(debug_cache, tokens, target_inspect_layer_num, src_idx, target_des_idx)\n",
    "            else:\n",
    "                for target_des_idx in range(len(tokens)):\n",
    "                    show_attention_infos(debug_cache, tokens, target_inspect_layer_num, src_idx, target_des_idx)\n",
    "                    \n",
    "        else:\n",
    "            # Find information for des_idx\n",
    "            top_values, top_indices = torch.topk(avg_attn_probs[src_idx], k=len(avg_attn_probs))\n",
    "            # top_idx = ((top_indices == des_idx).nonzero(as_tuple=True)[0])\n",
    "            top_idx = top_indices.cpu().numpy().tolist().index(des_idx)\n",
    "            print(f\"Top {top_idx+1}\")\n",
    "            show_attention_infos(debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "\n",
    "    show_basic_details(eval_result, debug_result, debug_cache, infer_result)\n",
    "    db_id = eval_result['db_id']\n",
    "    pred = eval_result['predicted']\n",
    "    gold = eval_result['gold']\n",
    "    decode_history = debug_result['history']\n",
    "    correct = eval_result['exact']\n",
    "    questions = debug_cache['question']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    tokens = questions + columns + tables\n",
    "    tokens_len = len(tokens)\n",
    "    # Attention flow\n",
    "    if target_key == \"attention_score\":\n",
    "        # Select target destination indices\n",
    "        if inspect_layer_num is None:\n",
    "            for target_inspect_layer_num in range(8):\n",
    "                show_top_attention_infos_for_layer(target_inspect_layer_num,\n",
    "                                                   debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "        else:\n",
    "            target_inspect_layer_num = inspect_layer_num\n",
    "            show_top_attention_infos_for_layer(target_inspect_layer_num,\n",
    "                                    debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "\n",
    "    elif target_key == 'draw_attention_prob':\n",
    "        # Draw input x input matrix\n",
    "        for target_inspect_layer_num in range(8):\n",
    "            attn_probs = torch.tensor(debug_cache[f'layer_{target_inspect_layer_num}_attn_probs'])\n",
    "            avg_attn_probs = attn_probs.sum(dim=0)\n",
    "            x = tokens\n",
    "            y = tokens\n",
    "            visualize_attention(avg_attn_probs, x, y, title=f'{prefix}_token-token_attention_prob_layer-{target_inspect_layer_num}', decimal=3, save_as_file=True)\n",
    "            \n",
    "    elif target_key == 'draw_word_embeddings':\n",
    "        # Draw input x input matrix\n",
    "        for target_inspect_layer_num in range(8):\n",
    "            word_sim = debug_cache[f'layer_{target_inspect_layer_num}_emb_sim_logits']\n",
    "            word_sim_prob = torch.softmax(torch.tensor(word_sim), dim=-1).squeeze(0)\n",
    "            avg_word_sim_prob = word_sim_prob.sum(dim=0)\n",
    "            x = tokens\n",
    "            y = tokens\n",
    "            visualize_attention(avg_word_sim_prob, x, y, title=f'{prefix}_token-token_word_prob_layer-{target_inspect_layer_num}', decimal=3, save_as_file=True)\n",
    "        \n",
    "    elif target_key == 'draw_relation_embeddings':\n",
    "        avg_bias_sim_probs = []\n",
    "        for target_inspect_layer_num in range(8):\n",
    "            ## 1. Draw input x input\n",
    "            ### 1.1 Create attention matrix: input x input\n",
    "            bias_sim = debug_cache[f'layer_{target_inspect_layer_num}_bias_sim_logits']\n",
    "            bias_sim_prob = torch.softmax(torch.tensor(bias_sim), dim=-1).squeeze(0)\n",
    "            avg_bias_sim_prob = bias_sim_prob.sum(dim=0)\n",
    "            avg_bias_sim_probs.append(avg_bias_sim_prob)\n",
    "        \n",
    "            ## 2. Draw input x relation\n",
    "            ### 2.1 Create attention matrix: input x relation\n",
    "            attention_matrix = torch.zeros(avg_bias_sim_prob.shape[0], len(RELATION_NAMES))\n",
    "            history = []\n",
    "            for src_idx in range(avg_bias_sim_prob.shape[0]):\n",
    "                for target_des_idx in range(bias_sim_prob.shape[-1]):\n",
    "                    relation_id = debug_cache['relation'][src_idx][target_des_idx]\n",
    "                    relation_name = RELATION_NAMES[relation_id]\n",
    "                    if relation_id not in history:\n",
    "                        history.append(relation_id)\n",
    "                    attention_matrix[src_idx][relation_id] = avg_bias_sim_prob[src_idx][target_des_idx]\n",
    "            \n",
    "            # Filter out unused relations\n",
    "            history = sorted(list(set(history)))\n",
    "            filtered_relation_names = [RELATION_NAMES[i] for i in history]\n",
    "            filtered_attention_matrix = torch.zeros(avg_bias_sim_prob.shape[0], len(history))\n",
    "            for source_idx in range(avg_bias_sim_prob.shape[0]):\n",
    "                for relation_id in debug_cache['relation'][source_idx]:\n",
    "                    idx = history.index(relation_id)\n",
    "                    filtered_attention_matrix[source_idx, idx] = attention_matrix[source_idx, relation_id]\n",
    "            \n",
    "            ### 2.2 Draw attention matrix\n",
    "            x = tokens\n",
    "            y = filtered_relation_names\n",
    "            visualize_attention(filtered_attention_matrix, x, y, title=f'{prefix}_token-relation_bias_prob_layer-{target_inspect_layer_num}', decimal=3, save_as_file=True)\n",
    "        \n",
    "        # Draw attention matrix\n",
    "        for layer_num, avg_bias_sim_prob in enumerate(avg_bias_sim_probs):\n",
    "            x = tokens\n",
    "            y = tokens\n",
    "            visualize_attention(avg_bias_sim_prob, x, y, title=f'{prefix}_token-token_bias_prob_layer-{layer_num}', decimal=3, save_as_file=True)\n",
    "        \n",
    "\n",
    "            \n",
    "    elif target_key == 'relation_embeddings':\n",
    "        # Get all relation ids used in src_idx\n",
    "        relation_ids = list(set(debug_cache['relation'][src_idx]))\n",
    "        \n",
    "        for inspect_layer_num in range(8):\n",
    "            attn_probs =  debug_cache[f'layer_{inspect_layer_num}_attn_probs']\n",
    "            sim_logits =  debug_cache[f'layer_{inspect_layer_num}_sim_logits']\n",
    "            word_sim = debug_cache[f'layer_{inspect_layer_num}_emb_sim_logits']\n",
    "            bias_sim = debug_cache[f'layer_{inspect_layer_num}_bias_sim_logits']\n",
    "            print(f\"layer_num:{inspect_layer_num}\")\n",
    "            bias_sim_prob = torch.softmax(torch.tensor(bias_sim), dim=-1).squeeze(0)[:, src_idx].data.cpu().numpy()\n",
    "\n",
    "            history = []\n",
    "            for target_des_idx in range(bias_sim_prob.shape[-1]):\n",
    "                relation_id = debug_cache['relation'][src_idx][target_des_idx]\n",
    "                relation_name = RELATION_NAMES[relation_id]\n",
    "                cnt_relation_id = np.count_nonzero(debug_cache['relation'][src_idx] == relation_id)\n",
    "                if relation_id not in history:\n",
    "                    history.append(relation_id)\n",
    "                    avg_prob = sum(bias_sim_prob[:, target_des_idx]) / bias_sim_prob.shape[0]\n",
    "                    print(f\"\\trelation_id: {relation_id} ({relation_name}) x {cnt_relation_id}\")\n",
    "                    print('\\t\\t\\t\\t{:.3f} |\\t'.format(avg_prob), end='')\n",
    "                    \n",
    "                    for value in bias_sim_prob[:, target_des_idx]:\n",
    "                        print(\"{:.3f}\\t\".format(value), end='')\n",
    "                    print('')\n",
    "\n",
    "    elif target_key  == 'relation_embeddings':\n",
    "        assert inspect_layer_num is not None, \"Inspect_layer_num should not be None\"\n",
    "        for key, value in debug_cache.items():\n",
    "            if inspect_layer_num == get_layer_idx(key):\n",
    "                if 'relation_k_embs' in key or 'relation_v_embs' in key:\n",
    "                    print(key)\n",
    "                    for relation_id, relation_emb in enumerate(value):\n",
    "                        print(f\"Relation: {relation_id} ({RELATION_NAMES[relation_id]}) scale: {get_vector_norm(relation_emb)}\")\n",
    "                    print('')\n",
    "\n",
    "    elif target_key in ['attention_flow']:\n",
    "        bert_attention = torch.stack(debug_cache['bert_attention']).squeeze(1)\n",
    "        bert_attention = bert_attention[:3]\n",
    "        bert_tokens = debug_cache['input_tokens']\n",
    "        flow_att_mat = get_attention_flow(bert_tokens, bert_attention)\n",
    "        sentence = ' '.join(bert_tokens[1:-1])\n",
    "        draw_attention_flow(sentence, 3, flow_att_mat, [10, 14])\n",
    "\n",
    "    elif target_key in ['semi_attention_flow']:\n",
    "        bert_attention = torch.stack(debug_cache['bert_attention']).squeeze(1)\n",
    "        bert_tokens = debug_cache['input_tokens']\n",
    "        joint_weight_matrix = cal_attention_flow(bert_attention)\n",
    "        ## Create label\n",
    "        # target_labels = list(map(to_str, bert_input))\n",
    "        # source_labels = target_labels\n",
    "        # visualize_attention(joint_weight_matrix, source_labels, target_labels, title='Bert attention flow')\n",
    "        \n",
    "        # Show top 10 words instead of visualization\n",
    "        # Top 7 index and value from weights tensor\n",
    "        # print_top_7(joint_weight_matrix, bert_input)\n",
    "        print_top_7_for_all_layer_for_target_word(bert_attention.sum(dim=1)/bert_attention.shape[1], bert_tokens, 3)\n",
    "\n",
    "    # More detail attention\n",
    "    else:\n",
    "        for key, value in debug_cache.items():\n",
    "            layer_idx = get_layer_idx(key)\n",
    "\n",
    "            # only show desired key\n",
    "            if \"_\".join(key.split('_')[2:]) != target_key:\n",
    "                continue\n",
    "\n",
    "            tmps = []\n",
    "            for tmp_des_idx in range(tokens_len):\n",
    "                if 'sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'attn_probs' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'emb_sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'bias_sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'relation_k_embs' in key:\n",
    "                    relation_idx = debug_cache[f'layer_{layer_idx}_relation'][src_idx][tmp_des_idx]\n",
    "                    target_value = get_vector_norm(value[relation_idx])\n",
    "                elif 'relation_v_embs' in key:\n",
    "                    relation_idx = debug_cache[f'layer_{layer_idx}_relation'][src_idx][tmp_des_idx]\n",
    "                    target_value = get_vector_norm(value[relation_idx])\n",
    "\n",
    "                print(f\"key:{key} src:{src_idx} ({tokens[src_idx]}) -> des:{tmp_des_idx} ({tokens[tmp_des_idx]}) value:{target_value}\")\n",
    "                tmps.append(np.expand_dims(target_value, axis=0))\n",
    "\n",
    "            # Draw attention\n",
    "            target_labels = list(map(to_str, [tokens[src_idx]]))\n",
    "            source_labels = list(map(to_str, tokens))\n",
    "            tmps = np.stack(tmps)\n",
    "            visualize_attention(np.stack(tmps), source_labels, target_labels, title=key)\n",
    "    \n",
    "    # Scale\n",
    "    \n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f524e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "def show_decoder_details(eval_result, debug_result, debug_cache, infer_result, inspect_step_idx=None):\n",
    "    show_basic_details(eval_result, debug_result, debug_cache, infer_result)\n",
    "    db_id = eval_result['db_id']\n",
    "    decode_history = debug_result['history']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    \n",
    "    if inspect_step_idx is None:\n",
    "        # Show all steps briefly\n",
    "        for step, step_info in enumerate(decode_history):\n",
    "            print(f\"Step: {step}\")\n",
    "            rule_left = step_info['rule_left']\n",
    "            choices = step_info['choices']\n",
    "            probs = step_info['probs']\n",
    "\n",
    "            # Decoder: action choices and probs\n",
    "            print(f\"rule_left: {rule_left}\")\n",
    "            print(f\"choices: {choices}\")\n",
    "            print(f\"probs: {['{:.2f}'.format(prob*100) for prob in probs]}\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        # Analyze decoding steps\n",
    "        for step, step_info in enumerate(decode_history):\n",
    "            if inspect_step_idx != None and inspect_step_idx != step:\n",
    "                continue\n",
    "\n",
    "            # For easy referencing\n",
    "            db_id = infer_result['db_id']\n",
    "            nl_toks = infer_result['question_toks']\n",
    "            ## Schema and memory\n",
    "            tables = [' '.join(item) for item in infer_result['schema']['tables']]\n",
    "            columns_no_table = [' '.join(item[:-1]) for item in infer_result['schema']['columns']]\n",
    "            memory = nl_toks+columns+tables\n",
    "            ## align_mat\n",
    "            mc_align_matrix = torch.tensor(debug_cache['m2c_align_mat'])\n",
    "            mt_align_matrix = torch.tensor(debug_cache['m2t_align_mat'])\n",
    "            ## action scores\n",
    "            decode_history = debug_result['history']\n",
    "            ## encoder attention weight for each layers\n",
    "\n",
    "            print(f\"Step: {step}\")\n",
    "            # For easy reference\n",
    "            rule_left = step_info['rule_left']\n",
    "            choices = step_info['choices']\n",
    "            probs = step_info['probs']\n",
    "\n",
    "            # Decoder: action choices and probs\n",
    "            print(f\"rule_left: {rule_left}\")\n",
    "            print(f\"choices: {choices}\")\n",
    "            print(f\"probs: {['{:.2f}'.format(prob*100) for prob in probs]}\")\n",
    "\n",
    "            # Decoder: hidden state - memory attention\n",
    "            dec_att = torch.tensor(step_info['att_probs'])\n",
    "            visualize_attention(dec_att.transpose(0,1), memory, ['hidden_state'],\n",
    "                                    title=\"Hidden state - Memory attention\")\n",
    "\n",
    "            # More info for column/table\n",
    "            if rule_left in ['column', 'table']:\n",
    "                    # Analyze align matrix\n",
    "                if rule_left == 'column':\n",
    "                    visualize_attention(mc_align_matrix, memory, columns_no_table, title=\"Memory - Column alignment\", decimal=3)\n",
    "                else:\n",
    "                    visualize_attention(mt_align_matrix, memory, tables, title=\"Memory - Table alignment\", decimal=3)\n",
    "                \n",
    "                # Decoder: memory-pointer probs\n",
    "                memory_pointer_probs = torch.tensor(step_info['memory_pointer_probs'])\n",
    "                visualize_attention(memory_pointer_probs.transpose(0, 1), \n",
    "                                    memory, ['hidden_state'], title='Memory pointer probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e081fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_0/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=0,join_cond=false/model_checkpoint-078000.pt\n",
      "loaded model has last_step:78000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_2/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=2,join_cond=false/best_model.pt\n",
      "loaded model has last_step:41600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_3/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=3,join_cond=false/best_model.pt\n",
      "loaded model has last_step:41600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_0/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=0,join_cond=false/model_checkpoint-018000.pt\n",
      "loaded model has last_step:18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_2/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=2,join_cond=false/model_checkpoint-025000.pt\n",
      "loaded model has last_step:25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_3/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=3,join_cond=false/model_checkpoint-026000.pt\n",
      "loaded model has last_step:26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_glove_run_no_join_cond_seed_0/bs=24,lr=7.4e-04,end_lr=0e0,seed=0,join_cond=false/model_checkpoint-020000.pt\n",
      "loaded model has last_step:20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_glove_run_no_join_cond_seed_1/bs=24,lr=7.4e-04,end_lr=0e0,seed=1,join_cond=false/model_checkpoint-040000.pt\n",
      "loaded model has last_step:40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'use_seq_elem_rules': True}, 'encoder_preproc': {'compute_cv_link': True, 'compute_sc_link': True, 'count_tokens_in_word_emb_for_vocab': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'max_count': 5000, 'min_freq': 4, 'save_path': 'data/spider_test/nl2code-glove,cv_link=true', 'word_emb': {'kind': '42B', 'lemmatize': True, 'name': 'glove'}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_glove_run_no_join_cond_seed_2/bs=24,lr=7.4e-04,end_lr=0e0,seed=2,join_cond=false/model_checkpoint-018000.pt\n",
      "loaded model has last_step:18000\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "warnings.filterwarnings('ignore')\n",
    "trained_bert_models = [load_model(seed, model_type='bert') for seed in model_seeds]\n",
    "trained_electra_models = [load_model(seed, model_type='electra') for seed in model_seeds]\n",
    "trained_glove_models = [load_model(seed, model_type='glove') for seed in [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "2c4daa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "DB connections: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 460.63it/s]\n",
      "test section: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "DB connections: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 459.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "DB connections:   0%|                                                                                                                      | 0/166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before summary writer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 457.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote eval results to logdir/spider_electra_run_no_join_cond_seed_0/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=0,join_cond=false/ie_dirs/electra_run_true_1-step_18000-eval-testing.json\n"
     ]
    }
   ],
   "source": [
    "# Get model to use\n",
    "target_seed = 0\n",
    "# model_type = 'glove'\n",
    "# model_type = 'bert'\n",
    "model_type = 'electra'\n",
    "\n",
    "infer_paths, debug_paths, eval_paths = get_info_paths(model_type, target_seed)\n",
    "if model_type == 'bert':\n",
    "    trained_models = trained_bert_models\n",
    "    model_idx = model_seeds.index(target_seed)\n",
    "elif model_type == 'electra':\n",
    "    trained_models = trained_electra_models\n",
    "    model_idx = model_seeds.index(target_seed)\n",
    "elif model_type == 'glove':\n",
    "    trained_models = trained_glove_models\n",
    "    model_idx = [0,1,2].index(target_seed)\n",
    "target_model, last_step = trained_models[model_idx]\n",
    "\n",
    "# Do inference on custom query\n",
    "db_id = 'pets_1'\n",
    "question = \"What are the ids of the students who do not own cat as pets?\"\n",
    "gold = \"SELECT stuid FROM student EXCEPT SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat'\"\n",
    "model_type=model_type\n",
    "cem = [] # Column exact match\n",
    "tem = [] # Table exact match\n",
    "cpm = [] # Column partial match\n",
    "tpm = [] # table partial match\n",
    "cm = [] # cell match\n",
    "nm = [] # number match\n",
    "dm = [] # date match\n",
    "cem_exclude=[]\n",
    "tem_exclude=[]\n",
    "cpm_exclude=[]\n",
    "test_info = TestInfo(question, gold, db_id, target_seed, model_type, cem, tem, cpm, tpm, cm, nm, dm, \n",
    "                                                         cem_exclude, tem_exclude, cpm_exclude)\n",
    "test_example(target_model, test_info, step=last_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e0ca6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis options\n",
    "analysis_type = ['attention_flow', 'relation_embeddings']\n",
    "              #   ['sim_logits', 'attn_probs', 'emb_sim_logits', 'bias_sim_logits',\n",
    "              # 'relation_k_embs', 'relation_v_embs',\n",
    "              # 'm2c_align_mat', 'm2t_align_mat']\n",
    "\n",
    "if model_type == 'glove':\n",
    "    model_idx = [0,1,2].index(target_seed)\n",
    "else:\n",
    "    model_idx = model_seeds.index(target_seed)\n",
    "# inspect_step_idx = 0\n",
    "inspect_layer_num = None\n",
    "# inspect_layer_num = 0\n",
    "# src_idx = 3\n",
    "# des_idx = 6\n",
    "# src_idx = 6\n",
    "# des_idx = 3\n",
    "# src_idx = 3\n",
    "# des_idx = 18\n",
    "src_idx = 18\n",
    "des_idx = 3\n",
    "# des_idx = None\n",
    "# target_key = 'attention_flow'\n",
    "target_key = 'attention_score'\n",
    "# target_key = 'relation_embeddings'\n",
    "# target_key = 'draw_relation_embeddings'\n",
    "# target_key = 'draw_word_embeddings'\n",
    "# target_key = 'draw_attention_prob'\n",
    "eval_result, debug_result, debug_cache, infer_result = get_info(eval_paths[model_idx],\n",
    "                                                                     debug_paths[model_idx],\n",
    "                                                                     infer_paths[model_idx],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5303bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GET COLUMN AND TABLE INDEX\n",
    "# show_schema_entity_indices(eval_result, debug_result, debug_cache, infer_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a4a9b776",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is_correct:True\n",
      "DB_id:pets_1\n",
      "NL:['what', 'are', 'the', 'id', '##s', 'of', 'the', 'students', 'who', 'do', 'not', 'own', 'cat', 'as', 'pets', '?']\n",
      "Pred:SELECT Student.StuID FROM Student EXCEPT SELECT Has_Pet.StuID FROM Has_Pet JOIN Pets ON Has_Pet.PetID = Pets.PetID WHERE Pets.PetType = 'terminal'\n",
      "Gold:SELECT stuid FROM student EXCEPT SELECT T1.stuid FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid  =  T2.stuid JOIN pets AS T3 ON T3.petid  =  T2.petid WHERE T3.pettype  =  'cat'\n",
      "\n",
      "Top 34\n",
      "layer_num:0\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.002  |\t0.000\t0.002\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t\n",
      "sim_score (word+bias):\n",
      "\t-0.788  |\t-3.182\t1.304\t2.940\t-2.410\t1.704\t-0.046\t1.475\t-2.574\t\n",
      "word_sim:\n",
      "\t11.061  |\t0.444\t3.338\t1.385\t3.380\t-4.491\t1.802\t4.010\t1.194\t\n",
      "bias_sim:\n",
      "\t-11.850  |\t-3.625\t-2.034\t1.555\t-5.791\t6.195\t-1.847\t-2.535\t-3.768\t\n",
      "word_sim_prob:\n",
      "\t0.078  |\t0.000\t0.060\t0.000\t0.000\t0.000\t0.004\t0.013\t0.000\t\n",
      "bias_sim_prob:\n",
      "\t0.130  |\t0.000\t0.001\t0.062\t0.000\t0.062\t0.004\t0.001\t0.000\t\n",
      "\n",
      "Top 4\n",
      "layer_num:1\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.796  |\t0.000\t0.000\t0.000\t0.796\t0.000\t0.000\t0.000\t0.000\t\n",
      "sim_score (word+bias):\n",
      "\t-148.017  |\t25.762\t-9.147\t-6.082\t68.191\t-42.088\t-71.017\t-98.372\t-15.263\t\n",
      "word_sim:\n",
      "\t-100.340  |\t22.969\t4.395\t11.936\t66.128\t-45.462\t-57.390\t-94.784\t-8.131\t\n",
      "bias_sim:\n",
      "\t-47.677  |\t2.793\t-13.542\t-18.018\t2.063\t3.374\t-13.627\t-3.588\t-7.133\t\n",
      "word_sim_prob:\n",
      "\t0.821  |\t0.000\t0.000\t0.000\t0.821\t0.000\t0.000\t0.000\t0.000\t\n",
      "bias_sim_prob:\n",
      "\t0.035  |\t0.002\t0.000\t0.000\t0.030\t0.003\t0.000\t0.000\t0.000\t\n",
      "\n",
      "Top 24\n",
      "layer_num:2\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.061  |\t0.022\t0.008\t0.000\t0.000\t0.027\t0.000\t0.001\t0.003\t\n",
      "sim_score (word+bias):\n",
      "\t6.556  |\t2.586\t0.889\t-3.063\t-4.243\t2.127\t-2.294\t10.880\t-0.326\t\n",
      "word_sim:\n",
      "\t42.942  |\t2.895\t5.995\t4.212\t3.479\t5.598\t2.079\t15.731\t2.953\t\n",
      "bias_sim:\n",
      "\t-36.386  |\t-0.309\t-5.106\t-7.276\t-7.722\t-3.471\t-4.372\t-4.851\t-3.278\t\n",
      "word_sim_prob:\n",
      "\t0.171  |\t0.021\t0.042\t0.002\t0.000\t0.078\t0.014\t0.003\t0.010\t\n",
      "bias_sim_prob:\n",
      "\t0.077  |\t0.060\t0.005\t0.000\t0.001\t0.007\t0.000\t0.002\t0.002\t\n",
      "\n",
      "Top 13\n",
      "layer_num:3\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.116  |\t0.000\t0.003\t0.110\t0.000\t0.000\t0.000\t0.000\t0.001\t\n",
      "sim_score (word+bias):\n",
      "\t15.756  |\t-2.360\t3.870\t8.025\t-0.880\t0.249\t5.806\t0.333\t0.714\t\n",
      "word_sim:\n",
      "\t38.293  |\t1.594\t5.514\t8.329\t5.099\t3.517\t6.330\t3.937\t3.973\t\n",
      "bias_sim:\n",
      "\t-22.537  |\t-3.955\t-1.644\t-0.304\t-5.979\t-3.269\t-0.525\t-3.604\t-3.259\t\n",
      "word_sim_prob:\n",
      "\t0.291  |\t0.017\t0.028\t0.103\t0.022\t0.032\t0.051\t0.006\t0.030\t\n",
      "bias_sim_prob:\n",
      "\t0.045  |\t0.000\t0.000\t0.043\t0.000\t0.000\t0.000\t0.000\t0.001\t\n",
      "\n",
      "Top 13\n",
      "layer_num:4\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.200  |\t0.007\t0.179\t0.001\t0.001\t0.000\t0.000\t0.000\t0.011\t\n",
      "sim_score (word+bias):\n",
      "\t12.466  |\t6.608\t9.295\t0.629\t-3.041\t3.722\t-3.131\t1.271\t-2.887\t\n",
      "word_sim:\n",
      "\t27.150  |\t6.746\t6.400\t2.745\t1.051\t5.336\t0.536\t3.354\t0.983\t\n",
      "bias_sim:\n",
      "\t-14.684  |\t-0.138\t2.896\t-2.116\t-4.092\t-1.614\t-3.667\t-2.082\t-3.870\t\n",
      "word_sim_prob:\n",
      "\t0.382  |\t0.054\t0.064\t0.036\t0.027\t0.032\t0.060\t0.051\t0.059\t\n",
      "bias_sim_prob:\n",
      "\t0.070  |\t0.005\t0.057\t0.000\t0.001\t0.000\t0.000\t0.000\t0.006\t\n",
      "\n",
      "Top 7\n",
      "layer_num:5\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.347  |\t0.014\t0.001\t0.022\t0.010\t0.009\t0.075\t0.065\t0.151\t\n",
      "sim_score (word+bias):\n",
      "\t19.013  |\t0.941\t-3.429\t2.655\t-3.288\t2.490\t8.441\t4.993\t6.210\t\n",
      "word_sim:\n",
      "\t18.442  |\t0.301\t0.046\t3.019\t-1.256\t3.587\t7.131\t1.984\t3.630\t\n",
      "bias_sim:\n",
      "\t0.571  |\t0.640\t-3.475\t-0.364\t-2.032\t-1.097\t1.310\t3.009\t2.580\t\n",
      "word_sim_prob:\n",
      "\t0.359  |\t0.033\t0.014\t0.059\t0.042\t0.042\t0.053\t0.034\t0.084\t\n",
      "bias_sim_prob:\n",
      "\t0.201  |\t0.007\t0.000\t0.009\t0.009\t0.006\t0.056\t0.059\t0.055\t\n",
      "\n",
      "Top 13\n",
      "layer_num:6\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.238  |\t0.114\t0.018\t0.011\t0.040\t0.015\t0.017\t0.024\t0.000\t\n",
      "sim_score (word+bias):\n",
      "\t13.886  |\t3.221\t2.207\t3.281\t2.139\t0.932\t-1.555\t2.628\t1.033\t\n",
      "word_sim:\n",
      "\t15.788  |\t3.808\t3.372\t3.317\t1.533\t1.540\t-0.903\t2.572\t0.548\t\n",
      "bias_sim:\n",
      "\t-1.901  |\t-0.587\t-1.164\t-0.036\t0.606\t-0.607\t-0.653\t0.055\t0.485\t\n",
      "word_sim_prob:\n",
      "\t0.360  |\t0.059\t0.145\t0.021\t0.024\t0.053\t0.028\t0.030\t0.001\t\n",
      "bias_sim_prob:\n",
      "\t0.160  |\t0.041\t0.003\t0.009\t0.048\t0.006\t0.021\t0.024\t0.007\t\n",
      "\n",
      "Top 20\n",
      "layer_num:7\n",
      "i:18 (student.last_name) j:3 (id) r:7 (cq_default)\n",
      "softmax_prob:\n",
      "\t0.121  |\t0.041\t0.001\t0.000\t0.000\t0.027\t0.004\t0.023\t0.025\t\n",
      "sim_score (word+bias):\n",
      "\t0.842  |\t-0.117\t1.352\t-0.522\t0.345\t0.398\t-0.392\t-1.410\t1.188\t\n",
      "word_sim:\n",
      "\t12.484  |\t0.158\t2.839\t1.221\t3.456\t0.812\t1.467\t-0.259\t2.791\t\n",
      "bias_sim:\n",
      "\t-11.641  |\t-0.274\t-1.487\t-1.743\t-3.111\t-0.414\t-1.859\t-1.151\t-1.602\t\n",
      "word_sim_prob:\n",
      "\t0.276  |\t0.033\t0.026\t0.033\t0.019\t0.031\t0.020\t0.029\t0.086\t\n",
      "bias_sim_prob:\n",
      "\t0.100  |\t0.036\t0.001\t0.000\t0.000\t0.027\t0.004\t0.023\t0.009\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do analysis\n",
    "show_encoder_details(eval_result, debug_result, debug_cache, infer_result, src_idx, des_idx, inspect_layer_num, target_key, top_k=None, prefix=f'seed_{target_seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b7592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_basic_details(eval_result, debug_result, debug_cache, infer_result)\n",
    "# db_id = eval_result['db_id']\n",
    "# pred = eval_result['predicted']\n",
    "# gold = eval_result['gold']\n",
    "# decode_history = debug_result['history']\n",
    "# correct = eval_result['exact']\n",
    "# questions = debug_cache['question']\n",
    "# input_columns = debug_cache['columns']\n",
    "# input_tables = debug_cache['tables']\n",
    "# columns = get_column_names(db_id)\n",
    "# tables = get_table_names(db_id)\n",
    "# assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "# assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "# tokens = questions + columns + tables\n",
    "# tokens_len = len(tokens)\n",
    "# bert_attention = torch.stack(debug_cache['bert_attention']).squeeze(1)\n",
    "# bert_attention = bert_attention[:2]\n",
    "# bert_tokens = debug_cache['input_tokens']\n",
    "# flow_att_mat = get_attention_flow(bert_tokens, bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f4320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = ' '.join(bert_tokens[1:-1])\n",
    "# draw_attention_flow(sentence, 3, flow_att_mat, [10, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e730bf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_inspect_step_idx = None\n",
    "# show_decoder_details(eval_result, debug_result, debug_cache, infer_result, inspect_step_idx=decoder_inspect_step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc8754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8fac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c71c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bb932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04081c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16018f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26a5743f5e35ae6571ca824f6fe42c87a3d3bd35a3dc9ed4a68e9bd849c38ffd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
