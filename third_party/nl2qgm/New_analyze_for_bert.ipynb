{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be242df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYLZE\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import warnings\n",
    "import _jsonnet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ratsql.commands.infer import Inferer\n",
    "from ratsql.utils.analysis import cal_attention_flow\n",
    "from search import read_data, match, show_results\n",
    "from run_all import test_example, load_model, TestInfo\n",
    "from ratsql.utils.relation_names import RELATION_NAMES\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1e759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "project_dir = \"/home/hkkang/NL2QGM\"\n",
    "\n",
    "# Trained model seeds\n",
    "model_seeds = [0, 2, 3]\n",
    "\n",
    "def get_info_paths(model_type, target_model_seed):\n",
    "    bert_template = \"logdir/spider_bert_run_no_join_cond_seed_{}/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed={},join_cond=false/ie_dirs/bert_run_true_1-step_41600-eval.json\"\n",
    "    electra_template = \"logdir/spider_electra_run_no_join_cond_seed_{}/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed={},join_cond=false/ie_dirs/electra_run_true_1-step_41600-eval.json\"\n",
    "    template = bert_template if model_type == 'bert' else electra_template\n",
    "    template = os.path.join(project_dir, template)\n",
    "\n",
    "    if model_type == 'electra':\n",
    "        if target_model_seed == 0:\n",
    "            template = template.replace('41600', '18000')\n",
    "        elif target_model_seed == 2:\n",
    "            template = template.replace('41600', '25000')\n",
    "        elif target_model_seed == 3:\n",
    "            template = template.replace('41600', '26000')\n",
    "    elif model_type == 'bert':\n",
    "        if target_model_seed == 0:\n",
    "            template = template.replace('41600', '78000')\n",
    "\n",
    "    eval_paths = []\n",
    "    for seed in model_seeds:\n",
    "        eval_paths += [template.format(seed, seed)]\n",
    "\n",
    "    infer_paths = [path.replace(\"-eval.json\", \"-infer.jsonl\") for path in eval_paths]\n",
    "    debug_paths = [path.replace(\"-eval.json\", \"-debug.jsonl\") for path in eval_paths]\n",
    "    return infer_paths, debug_paths, eval_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4038795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables.json\n",
    "tables_path = os.path.join(project_dir, \"data/spider/tables.json\")\n",
    "\n",
    "with open(tables_path) as f:\n",
    "    dbs = {item['db_id']: item for item in json.load(f)}\n",
    "\n",
    "def get_column_names(db_id):\n",
    "    db = dbs[db_id]\n",
    "    tables = db['table_names']\n",
    "    columns = []\n",
    "    for table_id, column_name in db['column_names']:\n",
    "        column_name = column_name.replace(' ', '_')\n",
    "        if table_id == -1:\n",
    "            columns.append(column_name)\n",
    "        else:\n",
    "            columns.append(f\"{tables[table_id].replace(' ', '_')}.{column_name}\")\n",
    "    return columns\n",
    "\n",
    "def get_table_names(db_id):\n",
    "    db = dbs[db_id]\n",
    "    tables = db['table_names']\n",
    "    return [table.replace(' ', '_') for table in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff5062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_json_custom(path):\n",
    "    result = json.load(open(path))['per_item']\n",
    "    return result\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as f:\n",
    "        results = [json.loads(line) for line in f.readlines()]\n",
    "    return results\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_info(eval_path, debug_path, infer_path):\n",
    "    eval_result = load_json_custom(eval_path.replace('.json', '-testing.json'))[0]\n",
    "    debug_cache = load_pickle(debug_path.replace('.jsonl', '-testing.pkl'))[0]\n",
    "    debug_result = load_jsonl(debug_path.replace('.jsonl', '-testing.jsonl'))[0]\n",
    "    infer_result = load_jsonl(infer_path.replace('.jsonl', '-testing.jsonl'))[0]\n",
    "\n",
    "    return eval_result, debug_result, debug_cache, infer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf3a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(mma, target_labels, source_labels, title=None, decimal=4):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mma: nxn weight matrix\n",
    "        source_labels: List of column labels\n",
    "        target_labels: List of row labels\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20,20), dpi=100)\n",
    "    im = ax.imshow(mma)\n",
    "    \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(mma.shape[1]), minor=False) # mma.shape[1] = target seq 길이\n",
    "    ax.set_yticks(np.arange(mma.shape[0]), minor=False) # mma.shape[0] = input seq 길이\n",
    "   \n",
    "    # source words -> column labels\n",
    "    ax.set_xticklabels(source_labels, minor=False)\n",
    "    # target words -> row labels\n",
    "    ax.set_yticklabels(target_labels, minor=False)\n",
    "  \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    " \n",
    "    plt.xticks(rotation=45)\n",
    " \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(mma)):\n",
    "        for j in range(len(mma[0])):\n",
    "            if decimal == 4:\n",
    "                text = ax.text(j, i, \"{:.4f}\".format(mma[i, j].item()), ha=\"center\", va=\"center\", color=\"k\")\n",
    "            else:\n",
    "                text = ax.text(j, i, \"{:.3f}\".format(mma[i, j].item()), ha=\"center\", va=\"center\", color=\"k\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ae82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_schema_entity_indices(eval_result, debug_result, debug_cache, infer_result):\n",
    "    db_id = eval_result['db_id']\n",
    "    questions = debug_cache['question']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    tokens = questions + columns + tables\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx < len(questions):\n",
    "            continue\n",
    "        if idx < (len(questions) + len(columns)):\n",
    "            entity_type = 'column'\n",
    "        else:\n",
    "            entity_type = 'table'\n",
    "        print(f\"idx:{idx}\\t{entity_type}:\\t{token}\")\n",
    "        \n",
    "def get_vector_norm(vector):\n",
    "    try:\n",
    "        return np.linalg.norm(vector)\n",
    "    except:\n",
    "        return torch.linalg.norm(vector)\n",
    "    \n",
    "def show_basic_details(eval_result, debug_result, debug_cache, infer_result):\n",
    "    db_id = eval_result['db_id']\n",
    "    nl = infer_result['question_toks']\n",
    "    pred = eval_result['predicted']\n",
    "    gold = eval_result['gold']\n",
    "    decode_history = debug_result['history']\n",
    "    correct = eval_result['exact']\n",
    "    print(f\"Is_correct:{bool(correct)}\")\n",
    "    print(f\"DB_id:{db_id}\")\n",
    "    print(f\"NL:{nl}\")\n",
    "    print(f\"Pred:{pred}\")\n",
    "    print(f\"Gold:{gold}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5460d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_encoder_details(eval_result, debug_result, debug_cache, infer_result, src_idx, des_idx, inspect_layer_num, target_key, top_k=10):\n",
    "    def get_layer_idx(key):\n",
    "        if 'layer' in key:\n",
    "            return int(key.split('_')[1])\n",
    "        return None\n",
    "        \n",
    "    def to_str(value):\n",
    "        if type(value) == str:\n",
    "            return value\n",
    "        elif type(value) == list:\n",
    "            return ' '.join(value)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Bad type:{value}\")\n",
    "\n",
    "    def print_top_7(joint_weight_matrix, input_tokens):\n",
    "        \"\"\"\n",
    "        joint_weight_matrix: [input_num, output_num]\n",
    "        \"\"\"\n",
    "        top_7_values, top_7_indices = torch.topk(joint_weight_matrix, k=7)\n",
    "        for cur_idx, (top_7_index, top_7_value) in enumerate(zip(top_7_indices, top_7_values)):\n",
    "            print(f\"idx:{cur_idx} word:{input_tokens[cur_idx]}\")\n",
    "            for top_idx, (item_idx, weight_value) in enumerate(zip(top_7_index, top_7_value)):\n",
    "                print(f\"\\ttop{top_idx+1}_idx:{item_idx} (word:{input_tokens[item_idx]}) {weight_value}\")\n",
    "\n",
    "    def print_top_7_for_all_layer_for_target_word(joint_weight_matrix, input_tokens, target_word_idx):\n",
    "        \"\"\"\n",
    "        joint_weight_matrix: [layer_num, input_num, output_num]\n",
    "        \"\"\"\n",
    "        joint_weight_matrix = torch.tensor(joint_weight_matrix)[:,target_word_idx,:]\n",
    "        print(joint_weight_matrix.shape)\n",
    "        top_7_values, top_7_indices = torch.topk(joint_weight_matrix, k=7)\n",
    "        for cur_idx, (top_7_index, top_7_value) in enumerate(zip(top_7_indices, top_7_values)):\n",
    "            print(f\"idx:{cur_idx} word:{input_tokens[target_word_idx]}\")\n",
    "            for top_idx, (item_idx, weight_value) in enumerate(zip(top_7_index, top_7_value)):\n",
    "                print(item_idx)\n",
    "                print(f\"\\ttop{top_idx+1}_idx:{item_idx} (word:{input_tokens[item_idx]}) {weight_value}\")\n",
    "\n",
    "                \n",
    "    def print_float(prefix, tensor):\n",
    "        assert len(tensor.shape) == 1, f\"bad: {tensor.shape}\"\n",
    "        print(prefix)\n",
    "        print(f'\\t', end='')\n",
    "        avg = sum(tensor) / len(tensor)\n",
    "        print(\"{:.3f}  |\\t\".format(avg), end='')\n",
    "        for value in tensor:\n",
    "            print(\"{:.3f}\\t\".format(value), end='')\n",
    "        print(\"\")\n",
    "\n",
    "    def show_attention_infos(debug_cache, tokens, inspect_layer_num, src_idx, des_idx):\n",
    "        # Show \\alpha_{ij}, e_{ij}, embedding_sim, relation_bias_sim\n",
    "        attn_probs =  debug_cache[f'layer_{inspect_layer_num}_attn_probs']\n",
    "        sim_logits =  debug_cache[f'layer_{inspect_layer_num}_sim_logits']\n",
    "        word_sim = debug_cache[f'layer_{inspect_layer_num}_emb_sim_logits']\n",
    "        bias_sim = debug_cache[f'layer_{inspect_layer_num}_bias_sim_logits']\n",
    "        relation_id = debug_cache['relation'][src_idx][des_idx]\n",
    "        relation_name = RELATION_NAMES[relation_id]\n",
    "        print(f\"layer_num:{inspect_layer_num}\")\n",
    "        print(f\"i:{src_idx} ({tokens[src_idx]}) j:{des_idx} ({tokens[des_idx]}) r:{relation_id} ({relation_name})\")\n",
    "        print_float(\"softmax_prob:\", attn_probs[:, src_idx, des_idx])\n",
    "        print_float(\"sim_score (word+bias):\", sim_logits[:, src_idx, des_idx])\n",
    "        print_float(\"word_sim:\", word_sim[:, src_idx, des_idx])\n",
    "        print_float(\"bias_sim:\", bias_sim[:, src_idx, des_idx])\n",
    "        print_float(\"word_sim_prob:\", torch.softmax(torch.tensor(word_sim), dim=-1)[:, src_idx, des_idx])\n",
    "        print_float(\"bias_sim_prob:\", torch.softmax(torch.tensor(bias_sim), dim=-1)[:, src_idx, des_idx])\n",
    "        print(\"\")\n",
    "\n",
    "    def show_top_attention_infos_for_layer(target_inspect_layer_num, debug_cache, tokens, inspect_layer_num, src_idx, des_idx):\n",
    "        attn_probs = torch.tensor(debug_cache[f'layer_{target_inspect_layer_num}_attn_probs'])\n",
    "        avg_attn_probs = attn_probs.mean(dim=0)\n",
    "        if des_idx is None:\n",
    "            # Find top-10 most similary des_indices\n",
    "            top_values, top_indices = torch.topk(avg_attn_probs[src_idx], k=min(top_k, len(avg_attn_probs)))\n",
    "            des_indices = top_indices\n",
    "            # Show attention info for all target destination indices\n",
    "            for top_idx, target_des_idx in enumerate(des_indices):\n",
    "                print(f\"Top {top_idx+1}\")\n",
    "                show_attention_infos(debug_cache, tokens, target_inspect_layer_num, src_idx, target_des_idx)\n",
    "        else:\n",
    "            # Find information for des_idx\n",
    "            top_values, top_indices = torch.topk(avg_attn_probs[src_dix], k=len(attn_probs))\n",
    "            # top_idx = ((top_indices == des_idx).nonzero(as_tuple=True)[0])\n",
    "            top_idx = top_indices.cpu().numpy().tolist().index(des_idx)\n",
    "            print(f\"Top {top_idx+1}\")\n",
    "            show_attention_infos(debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "\n",
    "    show_basic_details(eval_result, debug_result, debug_cache, infer_result)\n",
    "    db_id = eval_result['db_id']\n",
    "    pred = eval_result['predicted']\n",
    "    gold = eval_result['gold']\n",
    "    decode_history = debug_result['history']\n",
    "    correct = eval_result['exact']\n",
    "    questions = debug_cache['question']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    tokens = questions + columns + tables\n",
    "    tokens_len = len(tokens)\n",
    "    # Attention flow\n",
    "    if target_key == \"attention_score\":\n",
    "        # Select target destination indices\n",
    "        if inspect_layer_num is None:\n",
    "            for target_inspect_layer_num in range(8):\n",
    "                show_top_attention_infos_for_layer(target_inspect_layer_num,\n",
    "                                                   debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "        else:\n",
    "            target_inspect_layer_num = inspect_layer_num\n",
    "            show_top_attention_infos_for_layer(target_inspect_layer_num,\n",
    "                                    debug_cache, tokens, target_inspect_layer_num, src_idx, des_idx)\n",
    "\n",
    "    elif target_key == 'relation_embeddings':\n",
    "        # Get all relation ids used in src_idx\n",
    "        relation_ids = list(set(debug_cache['relation'][src_idx]))\n",
    "        \n",
    "        for inspect_layer_num in range(8):\n",
    "            attn_probs =  debug_cache[f'layer_{inspect_layer_num}_attn_probs']\n",
    "            sim_logits =  debug_cache[f'layer_{inspect_layer_num}_sim_logits']\n",
    "            word_sim = debug_cache[f'layer_{inspect_layer_num}_emb_sim_logits']\n",
    "            bias_sim = debug_cache[f'layer_{inspect_layer_num}_bias_sim_logits']\n",
    "            print(f\"layer_num:{inspect_layer_num}\")\n",
    "            bias_sim_prob = torch.softmax(torch.tensor(bias_sim), dim=-1).squeeze(0)[:, src_idx].data.cpu().numpy()\n",
    "\n",
    "            history = []\n",
    "            for target_des_idx in range(bias_sim_prob.shape[-1]):\n",
    "                relation_id = debug_cache['relation'][src_idx][target_des_idx]\n",
    "                relation_name = RELATION_NAMES[relation_id]\n",
    "                if relation_id not in history:\n",
    "                    history.append(relation_id)\n",
    "                    print(f\"\\trelation_id: {relation_id} ({relation_name}) \")\n",
    "                    print('\\t\\t\\t\\t\\t', end='')\n",
    "                    for value in bias_sim_prob[:, target_des_idx]:\n",
    "                        print(\"{:.3f}\\t\".format(value), end='')\n",
    "                    print('')\n",
    "            \n",
    "    elif target_key  == 'relation_embeddings':\n",
    "        assert inspect_layer_num is not None, \"Inspect_layer_num should not be None\"\n",
    "        for key, value in debug_cache.items():\n",
    "            if inspect_layer_num == get_layer_idx(key):\n",
    "                if 'relation_k_embs' in key or 'relation_v_embs' in key:\n",
    "                    print(key)\n",
    "                    for relation_id, relation_emb in enumerate(value):\n",
    "                        print(f\"Relation: {relation_id} ({RELATION_NAMES[relation_id]}) scale: {get_vector_norm(relation_emb)}\")\n",
    "                    print('')\n",
    "\n",
    "    elif target_key in ['attention_flow']:\n",
    "        bert_attention = torch.stack(debug_cache['bert_attention']).squeeze(1)\n",
    "        bert_attention = bert_attention[:3]\n",
    "        bert_tokens = debug_cache['input_tokens']\n",
    "        flow_att_mat = get_attention_flow(bert_tokens, bert_attention)\n",
    "        sentence = ' '.join(bert_tokens[1:-1])\n",
    "        draw_attention_flow(sentence, 3, flow_att_mat, [10, 14])\n",
    "\n",
    "    elif target_key in ['semi_attention_flow']:\n",
    "        bert_attention = torch.stack(debug_cache['bert_attention']).squeeze(1)\n",
    "        bert_tokens = debug_cache['input_tokens']\n",
    "        joint_weight_matrix = cal_attention_flow(bert_attention)\n",
    "        ## Create label\n",
    "        # target_labels = list(map(to_str, bert_input))\n",
    "        # source_labels = target_labels\n",
    "        # visualize_attention(joint_weight_matrix, source_labels, target_labels, title='Bert attention flow')\n",
    "        \n",
    "        # Show top 10 words instead of visualization\n",
    "        # Top 7 index and value from weights tensor\n",
    "        # print_top_7(joint_weight_matrix, bert_input)\n",
    "        print_top_7_for_all_layer_for_target_word(bert_attention.sum(dim=1)/bert_attention.shape[1], bert_tokens, 3)\n",
    "\n",
    "    # More detail attention\n",
    "    else:\n",
    "        for key, value in debug_cache.items():\n",
    "            layer_idx = get_layer_idx(key)\n",
    "\n",
    "            # only show desired key\n",
    "            if \"_\".join(key.split('_')[2:]) != target_key:\n",
    "                continue\n",
    "\n",
    "            tmps = []\n",
    "            for tmp_des_idx in range(tokens_len):\n",
    "                if 'sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'attn_probs' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'emb_sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'bias_sim_logits' in key:\n",
    "                    target_value = value[src_idx][tmp_des_idx]\n",
    "                elif 'relation_k_embs' in key:\n",
    "                    relation_idx = debug_cache[f'layer_{layer_idx}_relation'][src_idx][tmp_des_idx]\n",
    "                    target_value = get_vector_norm(value[relation_idx])\n",
    "                elif 'relation_v_embs' in key:\n",
    "                    relation_idx = debug_cache[f'layer_{layer_idx}_relation'][src_idx][tmp_des_idx]\n",
    "                    target_value = get_vector_norm(value[relation_idx])\n",
    "\n",
    "                print(f\"key:{key} src:{src_idx} ({tokens[src_idx]}) -> des:{tmp_des_idx} ({tokens[tmp_des_idx]}) value:{target_value}\")\n",
    "                tmps.append(np.expand_dims(target_value, axis=0))\n",
    "\n",
    "            # Draw attention\n",
    "            target_labels = list(map(to_str, [tokens[src_idx]]))\n",
    "            source_labels = list(map(to_str, tokens))\n",
    "            tmps = np.stack(tmps)\n",
    "            visualize_attention(np.stack(tmps), source_labels, target_labels, title=key)\n",
    "    \n",
    "    # Scale\n",
    "    \n",
    "    #\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b0758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "def show_decoder_details(eval_result, debug_result, debug_cache, infer_result, inspect_step_idx=None):\n",
    "    show_basic_details(eval_result, debug_result, debug_cache, infer_result)\n",
    "    db_id = eval_result['db_id']\n",
    "    decode_history = debug_result['history']\n",
    "    input_columns = debug_cache['columns']\n",
    "    input_tables = debug_cache['tables']\n",
    "    columns = get_column_names(db_id)\n",
    "    tables = get_table_names(db_id)\n",
    "    assert len(columns) == len(input_columns), f\"{len(columns)} vs {len(input_columns)}\"\n",
    "    assert len(tables) == len(input_tables), f\"{len(tables)} vs {len(input_tables)}\"\n",
    "    \n",
    "    if inspect_step_idx is None:\n",
    "        # Show all steps briefly\n",
    "        for step, step_info in enumerate(decode_history):\n",
    "            print(f\"Step: {step}\")\n",
    "            rule_left = step_info['rule_left']\n",
    "            choices = step_info['choices']\n",
    "            probs = step_info['probs']\n",
    "\n",
    "            # Decoder: action choices and probs\n",
    "            print(f\"rule_left: {rule_left}\")\n",
    "            print(f\"choices: {choices}\")\n",
    "            print(f\"probs: {['{:.2f}'.format(prob*100) for prob in probs]}\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        # Analyze decoding steps\n",
    "        for step, step_info in enumerate(decode_history):\n",
    "            if inspect_step_idx != None and inspect_step_idx != step:\n",
    "                continue\n",
    "\n",
    "            # For easy referencing\n",
    "            db_id = infer_result['db_id']\n",
    "            nl_toks = infer_result['question_toks']\n",
    "            ## Schema and memory\n",
    "            tables = [' '.join(item) for item in infer_result['schema']['tables']]\n",
    "            columns_no_table = [' '.join(item[:-1]) for item in infer_result['schema']['columns']]\n",
    "            memory = nl_toks+columns+tables\n",
    "            ## align_mat\n",
    "            mc_align_matrix = torch.tensor(debug_cache['m2c_align_mat'])\n",
    "            mt_align_matrix = torch.tensor(debug_cache['m2t_align_mat'])\n",
    "            ## action scores\n",
    "            decode_history = debug_result['history']\n",
    "            ## encoder attention weight for each layers\n",
    "\n",
    "            print(f\"Step: {step}\")\n",
    "            # For easy reference\n",
    "            rule_left = step_info['rule_left']\n",
    "            choices = step_info['choices']\n",
    "            probs = step_info['probs']\n",
    "\n",
    "            # Decoder: action choices and probs\n",
    "            print(f\"rule_left: {rule_left}\")\n",
    "            print(f\"choices: {choices}\")\n",
    "            print(f\"probs: {['{:.2f}'.format(prob*100) for prob in probs]}\")\n",
    "\n",
    "            # Decoder: hidden state - memory attention\n",
    "            dec_att = torch.tensor(step_info['att_probs'])\n",
    "            visualize_attention(dec_att.transpose(0,1), memory, ['hidden_state'],\n",
    "                                    title=\"Hidden state - Memory attention\")\n",
    "\n",
    "            # More info for column/table\n",
    "            if rule_left in ['column', 'table']:\n",
    "                    # Analyze align matrix\n",
    "                if rule_left == 'column':\n",
    "                    visualize_attention(mc_align_matrix, memory, columns_no_table, title=\"Memory - Column alignment\", decimal=3)\n",
    "                else:\n",
    "                    visualize_attention(mt_align_matrix, memory, tables, title=\"Memory - Table alignment\", decimal=3)\n",
    "                \n",
    "                # Decoder: memory-pointer probs\n",
    "                memory_pointer_probs = torch.tensor(step_info['memory_pointer_probs'])\n",
    "                visualize_attention(memory_pointer_probs.transpose(0, 1), \n",
    "                                    memory, ['hidden_state'], title='Memory pointer probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d23b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_0/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=0,join_cond=false/model_checkpoint-078000.pt\n",
      "loaded model has last_step:78000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_2/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=2,join_cond=false/best_model.pt\n",
      "loaded model has last_step:41600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'bert-large-uncased-whole-word-masking-finetuned-squad', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=bertsquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_bert_run_no_join_cond_seed_3/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=3,join_cond=false/best_model.pt\n",
      "loaded model has last_step:41600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_0/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=0,join_cond=false/model_checkpoint-018000.pt\n",
      "loaded model has last_step:18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_2/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=2,join_cond=false/model_checkpoint-025000.pt\n",
      "loaded model has last_step:25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': False, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bert_version': 'ahotrod/electra_large_discriminator_squad2_512', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/spider_test/database', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider_test/nl2code,join_cond=false,output_from=true,fs=2,emb=electrasquad,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logdir/spider_electra_run_no_join_cond_seed_3/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=3,join_cond=false/model_checkpoint-026000.pt\n",
      "loaded model has last_step:26000\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "warnings.filterwarnings('ignore')\n",
    "trained_bert_models = [load_model(seed, model_type='bert') for seed in model_seeds]\n",
    "trained_electra_models = [load_model(seed, model_type='electra') for seed in model_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37c2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "DB connections: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 327.81it/s]\n",
      "test section: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "WARNING <class 'ratsql.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n",
      "DB connections: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 455.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before summary writer...\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "Done writing summary!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 424.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote eval results to logdir/spider_bert_run_no_join_cond_seed_3/bs=8,lr=7.4e-04,bert_lr=1.0e-05,end_lr=0e0,seed=3,join_cond=false/ie_dirs/bert_run_true_1-step_41600-eval-testing.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get model to use\n",
    "target_seed = 3\n",
    "model_type = 'bert'\n",
    "# model_type = 'electra'\n",
    "\n",
    "infer_paths, debug_paths, eval_paths = get_info_paths(model_type, target_seed)\n",
    "trained_models = trained_bert_models if model_type == 'bert' else trained_electra_models\n",
    "model_idx = model_seeds.index(target_seed)\n",
    "target_model, last_step = trained_models[model_idx]\n",
    "\n",
    "# Do inference on custom query\n",
    "db_id = 'concert_singer'\n",
    "question = \"Show the name and the release year of the song by the youngest singer.\"\n",
    "gold = \"SELECT song_name ,  song_release_year FROM singer ORDER BY age LIMIT 1\"\n",
    "model_type=model_type\n",
    "cem = [] # Column exact match\n",
    "tem = [] # Table exact match\n",
    "cpm = [] # Column partial match\n",
    "tpm = [] # table partial match\n",
    "cm = [] # cell match\n",
    "nm = [] # number match\n",
    "dm = [] # date match\n",
    "cem_exclude=[]\n",
    "tem_exclude=[]\n",
    "cpm_exclude=[]\n",
    "test_info = TestInfo(question, gold, db_id, target_seed, model_type, cem, tem, cpm, tpm, cm, nm, dm, \n",
    "                                                         cem_exclude, tem_exclude, cpm_exclude)\n",
    "test_example(target_model, test_info, step=last_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cebf4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis options\n",
    "analysis_type = ['attention_flow', 'relation_embeddings']\n",
    "              #   ['sim_logits', 'attn_probs', 'emb_sim_logits', 'bias_sim_logits',\n",
    "              # 'relation_k_embs', 'relation_v_embs',\n",
    "              # 'm2c_align_mat', 'm2t_align_mat']\n",
    "\n",
    "model_idx = model_seeds.index(target_seed)\n",
    "# inspect_step_idx = 0\n",
    "# inspect_layer_num = None\n",
    "inspect_layer_num = 0\n",
    "src_idx = 2\n",
    "# des_idx = 24\n",
    "des_idx = None\n",
    "# target_key = 'attention_flow'\n",
    "# target_key = 'attention_score'\n",
    "target_key = 'relation_embeddings'\n",
    "eval_result, debug_result, debug_cache, infer_result = get_info(eval_paths[model_idx],\n",
    "                                                                     debug_paths[model_idx],\n",
    "                                                                     infer_paths[model_idx],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c211e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GET COLUMN AND TABLE INDEX\n",
    "# show_schema_entity_indices(eval_result, debug_result, debug_cache, infer_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b4da984",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is_correct:False\n",
      "DB_id:concert_singer\n",
      "NL:['show', 'the', 'name', 'and', 'the', 'release', 'year', 'of', 'the', 'song', 'by', 'the', 'youngest', 'singer', '.']\n",
      "Pred:SELECT singer.Name, singer.Song_release_year FROM singer ORDER BY singer.Age Asc LIMIT 1\n",
      "Gold:SELECT song_name ,  song_release_year FROM singer ORDER BY age LIMIT 1\n",
      "\n",
      "layer_num:0\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.098\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.000\t0.108\t0.000\t1.000\t0.009\t0.999\t1.000\t0.000\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.009\t0.000\t0.000\t0.154\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.000\t0.000\t0.091\t0.000\t0.000\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.050\t0.000\t0.000\t0.000\t0.049\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.000\t0.446\t0.000\t0.000\t0.000\t0.000\t0.000\t0.006\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.184\t\n",
      "layer_num:1\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.997\t0.000\t0.818\t0.000\t0.000\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.000\t0.975\t0.000\t0.000\t0.000\t0.001\t0.000\t0.000\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.035\t0.000\t0.445\t0.003\t0.000\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.000\t0.002\t0.000\t0.000\t0.091\t0.017\t0.000\t0.000\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.048\t0.000\t0.000\t0.000\t0.000\t0.000\t0.040\t0.000\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.000\t0.000\t0.278\t0.000\t0.000\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.049\t0.250\t\n",
      "layer_num:2\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.017\t0.705\t0.065\t0.595\t0.937\t0.000\t0.000\t0.000\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.040\t0.022\t0.005\t0.078\t0.002\t0.059\t0.023\t0.005\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.002\t0.001\t0.042\t0.003\t0.002\t0.016\t0.000\t0.000\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.004\t0.001\t0.010\t0.003\t0.017\t0.096\t0.002\t0.136\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.078\t0.008\t0.070\t0.004\t0.002\t0.039\t0.005\t0.060\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.002\t0.008\t0.005\t0.013\t0.000\t0.013\t0.044\t0.010\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.002\t0.007\t0.008\t0.001\t0.011\t0.004\t0.002\t0.001\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.008\t0.005\t0.000\t0.005\t0.000\t0.036\t0.006\t0.000\t\n",
      "layer_num:3\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.013\t0.082\t0.002\t0.068\t0.004\t0.002\t0.003\t0.258\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.003\t0.001\t0.004\t0.066\t0.001\t0.003\t0.003\t0.536\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.000\t0.052\t0.001\t0.134\t0.005\t0.005\t0.000\t0.006\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.053\t0.019\t0.081\t0.009\t0.006\t0.028\t0.001\t0.022\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.084\t0.074\t0.075\t0.027\t0.008\t0.085\t0.002\t0.004\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.000\t0.000\t0.002\t0.009\t0.001\t0.001\t0.046\t0.005\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.004\t0.002\t0.009\t0.071\t0.434\t0.001\t0.012\t0.006\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.000\t0.004\t0.008\t0.023\t0.004\t0.001\t0.008\t0.006\t\n",
      "layer_num:4\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.002\t0.066\t0.002\t0.002\t0.022\t0.001\t0.094\t0.001\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.004\t0.065\t0.001\t0.009\t0.015\t0.001\t0.035\t0.001\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.007\t0.002\t0.009\t0.040\t0.005\t0.002\t0.034\t0.000\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.016\t0.009\t0.009\t0.034\t0.010\t0.001\t0.025\t0.000\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.006\t0.045\t0.003\t0.020\t0.012\t0.016\t0.008\t0.088\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.001\t0.017\t0.000\t0.006\t0.039\t0.013\t0.001\t0.001\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.445\t0.011\t0.453\t0.279\t0.006\t0.000\t0.027\t0.001\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.001\t0.002\t0.009\t0.001\t0.006\t0.138\t0.162\t0.001\t\n",
      "layer_num:5\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.056\t0.040\t0.005\t0.003\t0.049\t0.025\t0.016\t0.003\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.001\t0.001\t0.582\t0.000\t0.026\t0.020\t0.005\t0.000\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.012\t0.006\t0.023\t0.001\t0.003\t0.003\t0.009\t0.001\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.007\t0.002\t0.152\t0.002\t0.012\t0.014\t0.051\t0.001\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.003\t0.022\t0.009\t0.057\t0.028\t0.084\t0.077\t0.067\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.043\t0.000\t0.000\t0.018\t0.000\t0.000\t0.003\t0.011\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.004\t0.352\t0.041\t0.000\t0.290\t0.004\t0.007\t0.005\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.009\t0.000\t0.013\t0.001\t0.006\t0.001\t0.000\t0.005\t\n",
      "layer_num:6\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.017\t0.018\t0.061\t0.004\t0.015\t0.047\t0.024\t0.000\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.030\t0.054\t0.026\t0.004\t0.008\t0.046\t0.129\t0.003\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.006\t0.127\t0.031\t0.043\t0.001\t0.002\t0.059\t0.001\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.009\t0.009\t0.090\t0.021\t0.007\t0.011\t0.096\t0.004\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.008\t0.036\t0.043\t0.022\t0.030\t0.071\t0.008\t0.088\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.028\t0.016\t0.005\t0.030\t0.019\t0.002\t0.013\t0.000\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.052\t0.032\t0.013\t0.031\t0.012\t0.002\t0.151\t0.010\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.048\t0.002\t0.047\t0.005\t0.060\t0.018\t0.009\t0.000\t\n",
      "layer_num:7\n",
      "\trelation_id: 0 (qq_dist_-2) \n",
      "\t\t\t\t\t0.238\t0.006\t0.002\t0.014\t0.001\t0.003\t0.000\t0.002\t\n",
      "\trelation_id: 1 (qq_dist_-1) \n",
      "\t\t\t\t\t0.087\t0.002\t0.004\t0.004\t0.001\t0.002\t0.009\t0.001\t\n",
      "\trelation_id: 2 (qq_dist_0) \n",
      "\t\t\t\t\t0.014\t0.001\t0.003\t0.058\t0.003\t0.003\t0.002\t0.003\t\n",
      "\trelation_id: 3 (qq_dist_1) \n",
      "\t\t\t\t\t0.012\t0.001\t0.001\t0.003\t0.002\t0.003\t0.017\t0.001\t\n",
      "\trelation_id: 4 (qq_dist_2) \n",
      "\t\t\t\t\t0.008\t0.013\t0.015\t0.063\t0.006\t0.003\t0.004\t0.088\t\n",
      "\trelation_id: 5 (qc_default) \n",
      "\t\t\t\t\t0.009\t0.042\t0.038\t0.006\t0.044\t0.001\t0.044\t0.001\t\n",
      "\trelation_id: 37 (qcCEM) \n",
      "\t\t\t\t\t0.099\t0.003\t0.020\t0.018\t0.004\t0.015\t0.000\t0.001\t\n",
      "\trelation_id: 6 (qt_default) \n",
      "\t\t\t\t\t0.044\t0.002\t0.008\t0.017\t0.010\t0.227\t0.008\t0.001\t\n"
     ]
    }
   ],
   "source": [
    "# Do analysis\n",
    "show_encoder_details(eval_result, debug_result, debug_cache, infer_result, src_idx, des_idx, inspect_layer_num, target_key, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d86b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inspect_step_idx = 9\n",
    "show_decoder_details(eval_result, debug_result, debug_cache, infer_result, inspect_step_idx=decoder_inspect_step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0b680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bb329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914ce3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6220225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a9d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9ee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26a5743f5e35ae6571ca824f6fe42c87a3d3bd35a3dc9ed4a68e9bd849c38ffd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
