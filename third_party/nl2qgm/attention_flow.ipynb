{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "rc={'font.size': 8, 'axes.labelsize': 8, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "mpl.rcParams['axes.linewidth'] = 0.0 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = BertTokenizer\n",
    "sentences = {}\n",
    "src = {}\n",
    "targets = {}\n",
    "sentences[1] = \"She asked the doctor about \"+tokenizer.mask_token+\" backache\"\n",
    "src[1] = 6\n",
    "targets[1] = (1,4) \n",
    "sentences[0] = \"He talked to her about his book\"\n",
    "src[0] = 6\n",
    "targets[0] = (1,4) \n",
    "sentences[2] = \"The author talked to Sara about \"+tokenizer.mask_token+\" book\"\n",
    "src[2] = 7\n",
    "targets[2] = (2,5) \n",
    "\n",
    "sentences[3] = \"John tried to convince Mary of his love and brought flowers for \"+tokenizer.mask_token\n",
    "src[3] = 13\n",
    "targets[3] = (1,5) \n",
    "\n",
    "sentences[4] = \"Mary convinced John of \"+tokenizer.mask_token+\" love\"\n",
    "src[4] = 5\n",
    "targets[4] = (1,3) \n",
    "\n",
    "ex_id = 2\n",
    "sentence = sentences[ex_id]\n",
    "tokens = ['[cls]']+tokenizer.tokenize(sentence)+['[sep]']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "print(tokenizer.decode(tf_input_ids))\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5229/1432211691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"She asked the doctor about \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" backache\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('venv_rat': virtualenv)"
  },
  "interpreter": {
   "hash": "26a5743f5e35ae6571ca824f6fe42c87a3d3bd35a3dc9ed4a68e9bd849c38ffd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}